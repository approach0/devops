[a0-publish]
exe = $SSH 'rsync -ru --delete --progress a0/demo/web/. /var/www/html/search'
dep[] = repo:a0-build
dep[] = httpd:http-update

[a0-demo-link]
exe = $SSH 'ln -sf /var/www/html/search /var/www/html/demo'
dep[] = deploy:a0-publish

[a0-index-upload]
exe = rsync -z --progress $INDEX_IMG $RSYNC_ADDR/$(basename $INDEX_IMG)
dep[] = rsyncd:list

[a0-ready-run]
# this script will also kill existing a0 search daemons
exe = $SSH 'bash -s' -- < $SCRIPT/a0-update-index-img.sh "$(basename $INDEX_IMG)" $RANK
dep[] = deploy:a0-demo-link
dep[] = deploy:a0-index-upload

[a0-master]
exe = $SSH "tmux new -d -s 'searchd' '$A0_MASTER_RUN'"
if = [ $RANK -eq 0 ]
dep[] = deploy:a0-ready-run

[a0-crawler-make-dir]
exe = $SSH "mkdir -p ~/corpus && mkdir -p ~/crawl-$RANK"

[a0-crawler-setup-dir]
exe = $SSH "cd ~/crawl-$RANK && cp ../a0/demo/crawler/*.py . && cp ../a0/demo/crawler/*.html . && ln -sf ~/corpus ./tmp"
dep[] = repo:a0-code
dep[] = deploy:a0-crawler-make-dir

[a0-crawler-modules]
exe = $SSH 'pip3 install BeautifulSoup4 pycurl'
dep[] = repo:base

[a0-crawler-kill]
exe = $SSH "tmux kill-session -t 'crawler-$RANK' || true"
dep[] = repo:base

[a0-crawler-run]
exe = $SSH "tmux new -d -s 'crawler-$RANK' 'cd ~/crawl-$RANK && ./$CRAWLER_RUN'"
dep[] = deploy:a0-crawler-setup-dir
dep[] = deploy:a0-crawler-kill
dep[] = deploy:a0-crawler-modules
